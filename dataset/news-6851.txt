


When Gordon Moore, a co-founder of Intel, first observed that “the number of components that could be etched onto the surface of a silicon wafer was doubling at regular intervals and would do so for the foreseeable future,” as John Markoff reports, it is entirely possible that no one quite understood the implications of what he was saying.

Over the next three decades, chip makers embarked on a seemingly endless miniaturization effort that made computers more powerful and more affordable. In 1965, the year Mr. Moore wrote what came to be known as Moore’s Law, the densest memory chips stored only about 1,000 bits of information. Today’s densest memory chips can hold roughly 20 billion bits.

It is not a stretch to say the world changed because transistors got smaller and cheaper.

But now that an influential association of chip makers is signaling a limit to their ability to keep making silicon chips smaller, the question is what’s next? There are possibilities, of course, like rethinking how computers work through a cutting-edge technology called quantum computing and making chips on other materials, like a type of carbon called graphene.

Today’s iPads, which cost a few hundred dollars, are more powerful than the mainframes of the 1980s, which would cost a few million dollars today. The industry hopes it will be able to keep up the improvements for another 30 years.


